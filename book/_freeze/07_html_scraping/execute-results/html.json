{
  "hash": "2da4b96eac038fd8a2b8cfbb28d17ff7",
  "result": {
    "engine": "jupyter",
    "markdown": "# 07_html_scraping.qmd\n\n---\ntitle: \"07 — HTML Scraping (Respect robots.txt)\"\nformat: html\nexecute:\n  echo: true\n---\n\nIntroduction — This page demonstrates a minimal, well-behaved HTML scraping pattern that first checks `robots.txt`, caches the fetched HTML, uses a clear `User-Agent`, and parses the page with BeautifulSoup. Explanations below focus on respectful scraping practices, the `robotparser` check, request headers/timeouts, polite pauses, caching, and basic parsing to extract the page title and links.\n\n::: {#a8c1953e .cell execution_count=1}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport time\nimport requests\nfrom urllib import robotparser\nfrom bs4 import BeautifulSoup\n\nURL = \"https://example.com\"\nRAW = Path(\"data/raw/page.html\")\nPath(\"data/raw\").mkdir(parents=True, exist_ok=True)\n\nrp = robotparser.RobotFileParser()\nrp.set_url(\"https://example.com/robots.txt\")\nrp.read()\nuser_agent = \"STAT4160-student/1.0\"\n\nif not rp.can_fetch(user_agent, URL):\n    raise SystemExit(\"robots.txt disallows scraping this URL.\")\n\nif RAW.exists():\n    html = RAW.read_text(encoding=\"utf-8\")\nelse:\n    res = requests.get(URL, headers={\"User-Agent\": user_agent}, timeout=10)\n    res.raise_for_status()\n    html = res.text\n    RAW.write_text(html, encoding=\"utf-8\")\n    time.sleep(1)\n\nsoup = BeautifulSoup(html, \"html.parser\")\ntitle = soup.title.get_text(strip=True) if soup.title else \"(no title)\"\nprint(\"Page title:\", title)\nlinks = [(a.get_text(strip=True), a.get(\"href\")) for a in soup.select(\"a[href]\")]\nprint(\"First 5 links:\", links[:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPage title: Example Domain\nFirst 5 links: [('Learn more', 'https://iana.org/domains/example')]\n```\n:::\n:::\n\n\nExplanation for the above code:\n\n- `robotparser.RobotFileParser()` / `set_url(...)` / `read()`  \n  Create a robots.txt parser instance, point it at the site's `robots.txt`, and load/parses its rules. This produces the rule set used to decide whether a particular `User-Agent` may fetch a particular URL.\n\n- `user_agent = \"STAT4160-student/1.0\"` and `rp.can_fetch(user_agent, URL)`  \n  Supply a clear, descriptive `User-Agent` that identifies the client. `can_fetch` checks whether the named user-agent is allowed to access the given URL per the site's `robots.txt`. If `can_fetch` returns `False`, the script exits immediately — scraping is aborted to respect the website's policy.\n\n- Caching with `RAW.exists()` / `RAW.write_text(...)`  \n  If a local copy of the page already exists at `data/raw/page.html`, the script reads the cached HTML instead of re-downloading. Caching reduces load on remote servers during development and repeated runs. When no cache exists, the script performs a single `GET` and writes the body to the cache file.\n\n- `requests.get(..., headers={\"User-Agent\": user_agent}, timeout=10)` and `res.raise_for_status()`  \n  The `User-Agent` header is sent with the request so servers can identify the client. `timeout=10` avoids hanging indefinitely on slow servers. `raise_for_status()` raises an exception for 4xx/5xx responses so failures are explicit and can be handled (or cause the script to exit under `set -e`).\n\n- `time.sleep(1)` — polite pause  \n  After fetching and saving a page, the script sleeps briefly to avoid sending requests in rapid succession. Respectful scraping typically includes pauses between requests; the appropriate delay depends on the target site and any `Crawl-delay` directives (see note below).\n\n- `BeautifulSoup(html, \"html.parser\")`, `soup.title`, and `soup.select(\"a[href]\")`  \n  Parse the HTML into a navigable DOM with BeautifulSoup. Extract the page title (if present) via `soup.title.get_text(...)`. Use `soup.select(\"a[href]\")` to select anchor tags that have an `href` attribute and build a list of `(text, href)` tuples for simple link extraction.\n\n- Safety and robustness notes:\n  - Always obey `robots.txt` as an explicit first step. `robotparser` helps automate this check, but be aware that `robots.txt` is advisory — responsible scrapers must still follow it.\n  - Some `robots.txt` files include `Crawl-delay` directives; Python's `robotparser` may not support all non-standard fields. If a `Crawl-delay` is present, respect it by increasing sleep intervals accordingly.\n  - Use descriptive `User-Agent` values and provide contact information when appropriate (e.g., `\"my-bot/1.0 (contact: email@example.com)\"`) so site operators can reach you if needed.\n  - Honor rate limits, avoid aggressive parallel requests, and prefer cached reads during development to minimize load on target sites.\n  - Handle relative URLs carefully when following links: use `urllib.parse.urljoin(base, href)` to form absolute URLs if you will fetch linked pages.\n\n```{bash}\nset -euo pipefail\npython3 scripts/scrape_example.py\n```\n\nExplanation for the above code:\n\n- `set -euo pipefail` makes the shell session fail-fast so that errors in the Python script are visible and stop subsequent commands.\n- `python3 scripts/scrape_example.py` runs the scraper. Under the checks in the Python code, the script will stop immediately if `robots.txt` disallows scraping the target URL.\n\n",
    "supporting": [
      "07_html_scraping_files"
    ],
    "filters": [],
    "includes": {}
  }
}