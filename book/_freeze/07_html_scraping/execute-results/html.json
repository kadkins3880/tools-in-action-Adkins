{
  "hash": "560dbf32bc7e28215a0bc18a59c9dbef",
  "result": {
    "engine": "jupyter",
    "markdown": "```yaml\n# 07_html_scraping.qmd\n---\ntitle: \"07 â€” HTML Scraping (Respect robots.txt)\"\nformat: html\nexecute:\n  echo: true\n---\n\n# Simple web scraping example with robots.txt check\n\nThis example fetches a page only if allowed by the site's `robots.txt`. It uses `urllib.robotparser`, `requests`, and `bs4`. Always obey robots.txt and website terms.\n\nimport urllib.robotparser\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\n\nsite = \"https://example.com\"\nrobots_url = urljoin(site, \"/robots.txt\")\n\nrp = urllib.robotparser.RobotFileParser()\nrp.set_url(robots_url)\nrp.read()\n\nuser_agent = \"*\"\npath = \"/\"\n\nif rp.can_fetch(user_agent, urljoin(site, path)):\n    res = requests.get(site)\n    print(\"Status code:\", res.status_code)\n    if res.status_code == 200:\n        soup = BeautifulSoup(res.text, \"html.parser\")\n        # Print the title tag text\n        title = soup.title.string if soup.title else \"(no title)\"\n        print(\"Page title:\", title)\nelse:\n    print(f\"Scraping disallowed by robots.txt for {site}\")\n\n",
    "supporting": [
      "07_html_scraping_files"
    ],
    "filters": [],
    "includes": {}
  }
}