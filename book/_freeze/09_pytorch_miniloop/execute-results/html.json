{
  "hash": "7973211553e51d2b5f2c6bc5a2cbabd1",
  "result": {
    "engine": "jupyter",
    "markdown": "# 09_pytorch_miniloop.qmd\n\n---\ntitle: \"09 — PyTorch Minimal Training Loop\"\nformat: html\nexecute:\n  echo: true\n---\n\nIntroduction — Minimal example showing how to implement a small supervised training loop in PyTorch. The snippet covers dataset creation, a tiny GRU-based model, device selection (CPU/GPU), optimizer and loss setup, and the standard training loop (forward, backward, gradient clipping, step). Explanations below describe the PyTorch-specific components and the role each plays in training.\n\n::: {#f046776d .cell execution_count=1}\n``` {.python .cell-code}\n# Minimal PyTorch example (runs on CPU if CUDA unavailable)\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\nT, F, N = 16, 3, 512\nX = np.random.normal(0, 0.5, size=(N, T, F)).astype(\"float32\")\nw = np.random.normal(0, 0.2, size=(F,)).astype(\"float32\")\ny = (X[:, -1, :] @ w + np.random.normal(0, 0.1, size=(N,))).astype(\"float32\")\n\nds = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\ndl = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n\nclass SmallGRU(nn.Module):\n    def __init__(self, F, H=32):\n        super().__init__()\n        self.gru = nn.GRU(F, H, batch_first=True)\n        self.head = nn.Linear(H, 1)\n    def forward(self, x):\n        _, h = self.gru(x)\n        return self.head(h[-1]).squeeze(-1)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet = SmallGRU(F).to(device)\nopt = torch.optim.AdamW(net.parameters(), lr=3e-3)\nloss_fn = nn.L1Loss()\n\nfor ep in range(1, 11):\n    net.train()\n    tot = 0.0\n    for xb, yb in dl:\n        xb, yb = xb.to(device), yb.to(device)\n        opt.zero_grad(set_to_none=True)\n        yhat = net(xb)\n        loss = loss_fn(yhat, yb)\n        loss.backward()\n        nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n        opt.step()\n        tot += loss.item() * xb.size(0)\n    print(f\"epoch {ep:02d} | train_mae={tot/len(ds):.6f}\")\n```\n:::\n\n\nExplanation for the above code:\n\n- `torch.manual_seed(0)` and `np.random.seed(0)`  \n  Set deterministic seeds for PyTorch and NumPy random generators to make the example reproducible during development and demos. Full determinism across platforms requires additional PyTorch flags and may still vary by backend.\n\n- Data creation and `TensorDataset` / `DataLoader`  \n  The example synthesizes features `X` and targets `y` with NumPy, converts them to PyTorch tensors, and wraps them in a `TensorDataset`. `DataLoader` provides minibatch iteration (`batch_size=64`) and shuffling (`shuffle=True`), which is essential for stochastic optimization.\n\n- `class SmallGRU(nn.Module):`  \n  Defines a compact model with a single GRU layer and a linear head. `nn.GRU(F, H, batch_first=True)` processes sequences with `F` features per time step and returns a hidden state of size `H`. The model's `forward` returns a single scalar per sample by taking the final hidden state (`h[-1]`) and applying `self.head`.\n\n- `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` and `.to(device)`  \n  Selects GPU if available; otherwise uses CPU. Moving the model and tensors to the same device is required for computation. This pattern makes the script portable across machines with and without CUDA.\n\n- `opt = torch.optim.AdamW(net.parameters(), lr=3e-3)`  \n  Uses the AdamW optimizer (Adam with decoupled weight decay) for parameter updates. AdamW is a sensible default for many models; learning rate and optimizer choice depend on the task.\n\n- `loss_fn = nn.L1Loss()`  \n  L1 loss computes mean absolute error (MAE) between predictions and targets. It is robust to outliers compared to squared losses and is a common choice for regression tasks where MAE is a meaningful metric.\n\n- Training loop structure:\n  - `net.train()` sets the module to training mode (affects layers like dropout or batch norm).\n  - For each batch: move inputs/targets to `device` with `.to(device)`.\n  - `opt.zero_grad(set_to_none=True)` clears gradients; `set_to_none=True` can be marginally faster and uses slightly less memory.\n  - `yhat = net(xb)` performs the forward pass.\n  - `loss = loss_fn(yhat, yb)` computes the batch loss.\n  - `loss.backward()` computes gradients via backpropagation.\n  - `nn.utils.clip_grad_norm_(net.parameters(), 1.0)` applies gradient clipping by global norm to avoid exploding gradients and stabilize training.\n  - `opt.step()` updates parameters using computed gradients.\n  - `tot += loss.item() * xb.size(0)` accumulates the sum of per-example losses to compute epoch MAE.\n\n- Epoch reporting: `tot/len(ds)`  \n  After iterating batches, dividing the accumulated (sum) loss by dataset size yields the mean absolute error across the epoch. Printing this each epoch provides a simple training progress metric.\n\n- Portability and small-scale notes:\n  - The example runs fine on CPU for demonstration and on GPU if available. For real training with larger models or data, prefer proper device-aware data loading, mixed precision, and more robust reproducibility settings.\n  - For deterministic behavior across runs and platforms, additional configuration is required (e.g., `torch.use_deterministic_algorithms(True)`), but this can reduce performance or rely on backend support.\n\n",
    "supporting": [
      "09_pytorch_miniloop_files"
    ],
    "filters": [],
    "includes": {}
  }
}