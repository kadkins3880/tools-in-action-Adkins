{
  "hash": "ce334fb777e4f447b497f578c0a3b2aa",
  "result": {
    "engine": "jupyter",
    "markdown": "# 06_pandas_joins_parquet.qmd\n\n---\ntitle: \"06 — pandas Joins & Parquet\"\nformat: html\nexecute:\n  echo: true\n---\n\nIntroduction — This page demonstrates using pandas to join a lookup (metadata) table into a processed DataFrame and writing the result to Parquet. Explanations focus on the pandas join/merge semantics, dtype adjustments for efficient storage, and writing Parquet files.\n\n::: {#67b68aeb .cell execution_count=1}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport pandas as pd\n\nPath(\"data/processed\").mkdir(parents=True, exist_ok=True)\ndf = pd.read_csv(\"data/processed/synth_features.csv\", parse_dates=[\"date\"])\nif \"weekday\" in df.columns:\n    df[\"weekday\"] = df[\"weekday\"].astype(\"int8\")\n\nmeta = pd.DataFrame({\n    \"weekday\": [1,2,3,4,5,6,7],\n    \"weekday_name\": [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n})\ndf = df.merge(meta, on=\"weekday\", how=\"left\")\n\nif \"price\" in df.columns:\n    df[\"price\"] = df[\"price\"].astype(\"float32\")\nif \"volume\" in df.columns:\n    df[\"volume\"] = df[\"volume\"].astype(\"int32\")\n\nout = \"data/processed/synth.parquet\"\ndf.to_parquet(out, index=False)\nprint(df.head())\nimport os\nprint(\"\\nParquet file size (bytes):\", os.path.getsize(out))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        date       price  volume  log_return      sma_5  weekday weekday_name\n0 2022-01-01  100.496696     748    0.000000  100.49670        6          Sat\n1 2022-01-02  100.358398     417   -0.001377  100.42755        7          Sun\n2 2022-01-03  101.006104     776    0.006433  100.62040        1          Mon\n3 2022-01-04  102.529198     324    0.014967  101.09760        2          Tue\n4 2022-01-05  102.294998     918   -0.002287  101.33708        3          Wed\n\nParquet file size (bytes): 7994\n```\n:::\n:::\n\n\nThe above code:\n\n- `pd.read_csv(..., parse_dates=[\"date\"])`  \n  Reads the CSV into a DataFrame and converts the `date` column to `datetime64[ns]`. Parsed datetimes enable correct sorting, time-based operations, and compact storage in Parquet.\n\n- `df[\"weekday\"] = df[\"weekday\"].astype(\"int8\")`  \n  Ensures the `weekday` column uses a small integer dtype before the join. Choosing a compact integer (`int8`) reduces memory use and results in smaller on-disk Parquet files.\n\n- `meta = pd.DataFrame({...})` and `df.merge(meta, on=\"weekday\", how=\"left\")`  \n  Creates a lookup table (`meta`) mapping weekday integers to names, then performs a **left join** of `df` with `meta` on the `weekday` column. A left join preserves all rows from the left table (`df`) and adds matching columns from `meta`; rows without a match receive `NaN` in the joined columns. Using `on=\"weekday\"` explicitly names the join key; `how=\"left\"` selects left-join semantics.\n\n- `df[\"price\"] = df[\"price\"].astype(\"float32\")` and `df[\"volume\"] = df[\"volume\"].astype(\"int32\")`  \n  Downcasts numeric columns to narrower dtypes to reduce memory and Parquet size. `float32` is typically sufficient for many price series and halves memory compared with `float64`. Choose dtypes that retain the required precision for downstream analysis.\n\n- `df.to_parquet(out, index=False)`  \n  Writes the DataFrame to a Parquet file. Parquet stores data column-wise and is efficient for both storage and columnar reads. `index=False` prevents pandas from writing the DataFrame index as a separate column, keeping the file tidy. Parquet writing depends on an available engine (commonly `pyarrow`); ensure an engine is installed in the environment when running this code.\n\n- `os.path.getsize(out)`  \n  Prints the Parquet file size in bytes as a quick round-trip check to confirm the file was written and to inspect storage impact after dtype optimizations.\n\n```{bash}\npython3 - <<'PY'\nfrom pathlib import Path\nimport pandas as pd\nPath(\"data/processed\").mkdir(parents=True, exist_ok=True)\nprint(\"Ensure data/processed/synth_features.csv exists, then run the Python snippet above in this page to create parquet.\")\nPY\n```\n\nThe inline shell/Python snippet above runs a short Python interpreter session (here-doc) that reminds the user to ensure the source CSV exists before running the main snippet. It does not perform joins or Parquet writes itself; it is a convenience check to avoid running the main snippet when the input file is missing.\n\n",
    "supporting": [
      "06_pandas_joins_parquet_files"
    ],
    "filters": [],
    "includes": {}
  }
}