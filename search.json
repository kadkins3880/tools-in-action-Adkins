[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tools-in-Action",
    "section": "",
    "text": "0.1 # index.qmd\ntitle: “Tools in Action — Quarto Book” author: “Kincayde Adkins” format: html execute: echo: true —",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tools in Action — Quarto Book</span>"
    ]
  },
  {
    "objectID": "01_git.html",
    "href": "01_git.html",
    "title": "2  01_git.qmd",
    "section": "",
    "text": "Introduction — This file shows a compact workflow for initializing a Git repository, configuring identity, making commits, creating and pushing a feature branch to GitHub, and tagging a release. Replace placeholder values (for example your-username, YourLastName, Your Name, and the email) with your actual values before running these commands.\ngit init\ngit branch -M main\ngit remote add origin https://github.com/your-username/tools-in-action-YourLastName.git\nThe above commands initialize a new Git repository in the current directory (git init), rename the current branch to main (git branch -M main forces the rename), and add a remote named origin that points to a GitHub repository at the specified URL (git remote add origin &lt;url&gt;). Replace the URL placeholders with your GitHub username and repository name.\ngit config user.name \"Your Name\"\ngit config user.email \"12345678+your_github_name@users.noreply.github.com\"\nThese commands set the committer identity for this repository. git config user.name sets the display name recorded in commits and git config user.email sets the email address recorded in commits. Use --global to set these values for all repositories on the machine.\ngit add .\ngit commit -m \"chore: scaffold Quarto book\"\ngit add . stages all new, modified, and deleted files from the current directory and its subdirectories for the next commit. git commit -m \"&lt;message&gt;\" creates a new commit with the staged changes and attaches the provided commit message. The example uses a Conventional Commit-style message but any message is valid.\ngit checkout -b feat/sql-chapter\ngit add book/05_sql_sqlite.qmd\ngit commit -m \"docs(sql): add initial join examples\"\ngit push -u origin feat/sql-chapter\ngit checkout -b &lt;branch&gt; creates a new branch and switches to it. git add &lt;path&gt; stages a specific file for commit. git commit -m \"&lt;message&gt;\" commits the staged change. git push -u origin &lt;branch&gt; pushes the new branch to the origin remote and sets the local branch to track the remote branch so future git push/git pull can be run without specifying the remote and branch.\ngit tag v1.0-tools-in-action\ngit push origin v1.0-tools-in-action\ngit tag &lt;name&gt; creates a tag at the current commit to mark a release or milestone. git push origin &lt;tag&gt; pushes that tag to the origin remote. To push all tags at once use git push origin --tags (not shown above).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>01 — Git & GitHub</span>"
    ]
  },
  {
    "objectID": "02_linux_basics.html",
    "href": "02_linux_basics.html",
    "title": "3  02_linux_basics.qmd",
    "section": "",
    "text": "Introduction — This page shows a Python data-generation script and the Linux shell commands used to run the script and inspect the resulting CSV file. Explanations below only cover the Linux commands (the topic); the Python script is shown for context but is not explained in detail.\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nPath(\"data/raw\").mkdir(parents=True, exist_ok=True)\n\nnp.random.seed(42)\nn = 100\ndf = pd.DataFrame({\n    \"date\": pd.date_range(start=\"2022-01-01\", periods=n, freq=\"D\"),\n    \"price\": (100 + np.cumsum(np.random.randn(n))).round(4),\n    \"volume\": np.random.randint(100, 1000, size=n)\n})\n\nout = Path(\"data/raw/synth_prices.csv\")\ndf.to_csv(out, index=False)\nprint(f\"Saved {out}\")\n\nSaved data\\raw\\synth_prices.csv\n\n\nNo Linux commands appear in the Python block above; it generates data/raw/synth_prices.csv for the subsequent shell commands.\nset -euo pipefail\npython3 scripts/make_synth_data.py\nset -euo pipefail is a common Bash safety idiom: - -e: exit immediately if any command returns a non-zero status. - -u: treat unset variables as an error and exit. - -o pipefail: cause a pipeline to fail if any command in the pipeline fails. Together they make shell scripts fail fast and avoid silent errors.\npython3 scripts/make_synth_data.py runs the Python script scripts/make_synth_data.py with the python3 interpreter found in the shell’s PATH.\nwc -l data/raw/synth_prices.csv\nhead -n 5 data/raw/synth_prices.csv\n\nwc -l &lt;file&gt; prints the number of lines in the file. Useful to confirm row count quickly (including the header line if present).\nhead -n 5 &lt;file&gt; prints the first five lines of the file. Useful to preview the header and first rows to verify the file’s structure and contents.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>02 — Linux Basics</span>"
    ]
  },
  {
    "objectID": "03_shell_essentials.html",
    "href": "03_shell_essentials.html",
    "title": "4  03_shell_essentials.qmd",
    "section": "",
    "text": "Introduction — This file demonstrates a Python feature-building script and the shell commands used to run it and perform quick, lightweight inspections. Explanations below cover only the shell commands.\n\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nPath(\"data/processed\").mkdir(parents=True, exist_ok=True)\ndf = pd.read_csv(\"data/raw/synth_prices.csv\", parse_dates=[\"date\"])\ndf = df.sort_values(\"date\").reset_index(drop=True)\ndf[\"log_return\"] = df[\"price\"].pct_change().apply(lambda x: np.log1p(x) if pd.notna(x) else 0.0)\ndf[\"sma_5\"] = df[\"price\"].rolling(5, min_periods=1).mean().round(6)\ndf[\"weekday\"] = df[\"date\"].dt.isocalendar().day.astype(\"int8\")\ndf.to_csv(\"data/processed/synth_features.csv\", index=False)\nprint(\"Wrote data/processed/synth_features.csv\")\n\nWrote data/processed/synth_features.csv\n\n\nset -euo pipefail\npython3 scripts/build_features.py\nThe set -euo pipefail line is a Bash safety idiom that makes scripts fail-fast and helps surface errors: - -e exits immediately if any command returns a non-zero status. - -u treats unset variables as an error and exits. - -o pipefail causes a pipeline to return a failure status if any command in the pipeline fails. Together they reduce silent failures in shell scripts.\npython3 scripts/build_features.py runs the Python script using the python3 interpreter found on PATH. When run under the set -euo pipefail settings, if the script exits with a non-zero status the shell will immediately stop execution.\necho \"Rows in processed file:\"\nwc -l data/processed/synth_features.csv || true\necho \"First 5 lines:\"\nhead -n 6 data/processed/synth_features.csv || true\n\necho \"Rows in processed file:\" prints a human-readable label to separate output sections.\nwc -l &lt;file&gt; prints the number of lines in the file. This includes the header row if the CSV has one, so the result is typically 1 + number_of_data_rows.\nhead -n 6 &lt;file&gt; prints the first 6 lines of the file. Using -n 6 is a common pattern to show the header plus the first 5 data rows for quick verification.\nThe || true appended to wc and head prevents the shell from exiting with a non-zero status when those commands fail (for example if the file does not exist). This is helpful when the surrounding script is run with set -e; it allows the inspection step to fail harmlessly while still showing any available output.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>03 — Shell Essentials</span>"
    ]
  },
  {
    "objectID": "04_makefile.html",
    "href": "04_makefile.html",
    "title": "5  04_makefile.qmd",
    "section": "",
    "text": "Introduction — This Makefile provides convenient targets for common development tasks: showing help, creating the environment, generating data, building a small local DB, producing feature files, rendering the Quarto book, running tests, and cleaning generated artifacts. Explanations below cover Makefile syntax and the behavior of each target.\n.DEFAULT_GOAL := help\n\n.PHONY: help env data db features book test clean\n\nhelp:\n    @echo \"Available targets:\"\n    @echo \"  env       Create virtualenv and install Python dependencies\"\n    @echo \"  data      Generate raw/synthetic data (runs scripts/make_synth_data.py)\"\n    @echo \"  db        Create/populate sqlite DB (runs scripts/make_sqlite.py)\"\n    @echo \"  features  Build feature files (runs scripts/build_features.py)\"\n    @echo \"  book      Render the Quarto book (quarto render book)\"\n    @echo \"  test      Run pytest\"\n    @echo \"  clean     Remove build artifacts and generated data\"\nExplanation for the above code: - .DEFAULT_GOAL := help sets the default target that runs when no target is specified; here it shows the help text. - .PHONY: ... declares phony targets that are not file names; this prevents conflicts if a file with the same name exists and ensures the recipes always run. - help: defines a target whose recipe prints the list of available targets. Each @echo prints a line; the leading @ suppresses echoing the command itself, so only the text is shown when make runs.\nenv:\n    pip install -r requirements.txt\ndata:\n    python scripts/make_synth_data.py\ndb:\n    python scripts/make_sqlite.py\nfeatures:\n    python scripts/build_features.py\nbook:\n    quarto render book\ntest:\n    pytest -q\nExplanation for the above code: - env: runs pip install -r requirements.txt to install Python dependencies. On systems where a virtual environment is preferred, create/activate the venv before calling this target or modify the recipe to create one. - data: runs the Python data-generation script scripts/make_synth_data.py to populate data/raw/. - db: runs scripts/make_sqlite.py to create/populate a local SQLite database (if that script exists). - features: runs scripts/build_features.py to create processed feature files (e.g., data/processed/). - book: runs quarto render book to render the Quarto project into the book/ output. - test: runs pytest -q to execute the test suite; -q makes pytest quieter in its output.\nclean:\nclean:\n    del /Q db\\*.db\n    del /Q data\\processed\\*\n    rmdir /S /Q book\\_freeze\nExplanation for the above code: - The clean target is intended to remove generated artifacts. In this Makefile the clean target appears twice (an empty definition followed by the recipe). Duplicate target definitions can be confusing — combine into a single clean: definition with its recipe for clarity. - The commands shown use Windows cmd/PowerShell syntax: - del /Q db\\*.db deletes .db files quietly. - del /Q data\\processed\\* deletes files in data\\processed. - rmdir /S /Q book\\_freeze removes the book_freeze directory and its contents quietly. - Portability notes: - del and rmdir are Windows commands and will not work in a Unix shell. If you run this Makefile under a Unix-like shell (bash, zsh, CI runners), replace these with portable Unix equivalents (for example rm -f db/*.db and rm -rf data/processed/* book/_freeze) or add conditional logic in the Makefile to detect the platform. - Recipes in Makefiles must be indented with a tab character, not spaces. Ensure your editor preserves tabs for recipe lines. - Suggestion: consolidate the duplicate clean target into one, and consider adding a cross-platform clean recipe or two separate targets (e.g., clean-windows and clean-unix) to avoid accidental failures on different systems.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>04 — Makefile</span>"
    ]
  },
  {
    "objectID": "05_sql_sqlite.html",
    "href": "05_sql_sqlite.html",
    "title": "6  05_sql_sqlite.qmd",
    "section": "",
    "text": "Introduction — This page shows a small Python script that creates a local SQLite database, writes two tables from pandas DataFrames (features and weekday_meta), runs a JOIN query using SQL, and prints the result. The follow-up shell commands show how to run the script. Explanations below focus on the SQLite usage from the Python code and the example SQL query.\n\nfrom pathlib import Path\nimport sqlite3\nimport pandas as pd\n\nPath(\"db\").mkdir(parents=True, exist_ok=True)\ncon = sqlite3.connect(\"db/synth.db\")\ntry:\n    df = pd.read_csv(\"data/processed/synth_features.csv\")\n    if \"weekday\" in df.columns:\n        df[\"weekday\"] = df[\"weekday\"].astype(int)\n    df.to_sql(\"features\", con, if_exists=\"replace\", index=False)\n\n    meta = pd.DataFrame({\n        \"weekday\": [1,2,3,4,5,6,7],\n        \"weekday_name\": [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n    })\n    meta.to_sql(\"weekday_meta\", con, if_exists=\"replace\", index=False)\n\n    q = \"\"\"\n    SELECT f.date, f.price, f.volume, f.log_return, f.sma_5, m.weekday_name\n    FROM features f JOIN weekday_meta m USING(weekday)\n    ORDER BY date LIMIT 10;\n    \"\"\"\n    print(pd.read_sql(q, con))\nfinally:\n    con.close()\n\n         date     price  volume  log_return      sma_5 weekday_name\n0  2022-01-01  100.4967     748    0.000000  100.49670          Sat\n1  2022-01-02  100.3584     417   -0.001377  100.42755          Sun\n2  2022-01-03  101.0061     776    0.006433  100.62040          Mon\n3  2022-01-04  102.5292     324    0.014967  101.09760          Tue\n4  2022-01-05  102.2950     918   -0.002287  101.33708          Wed\n5  2022-01-06  102.0609     333   -0.002291  101.64992          Thu\n6  2022-01-07  103.6401     783    0.015355  102.30626          Fri\n7  2022-01-08  104.4075     763    0.007377  102.98654          Sat\n8  2022-01-09  103.9381     926   -0.004506  103.26832          Sun\n9  2022-01-10  104.4806     473    0.005206  103.70544          Mon\n\n\nThe Python code and SQLite explanations:\n\nsqlite3.connect(\"db/synth.db\")\nOpens (or creates) a local SQLite database file at db/synth.db. The returned connection con is used for all subsequent SQL operations.\ndf = pd.read_csv(\"data/processed/synth_features.csv\") and df.to_sql(\"features\", con, if_exists=\"replace\", index=False)\nLoads the processed CSV into a pandas DataFrame and writes it to the SQLite database as a table named features. if_exists=\"replace\" overwrites any existing table with the same name; index=False prevents pandas from adding the DataFrame index as a separate column.\ndf[\"weekday\"] = df[\"weekday\"].astype(int)\nEnsures the weekday column uses an integer type compatible with SQL joins and the weekday_meta table.\nCreating the weekday_meta DataFrame and writing it to SQL (weekday_meta)\nThis small lookup table provides a human-readable weekday_name for join demonstrations. Storing reference/metadata tables in the same database is a common pattern for self-contained examples.\nThe SQL query string q\nThe query selects date, price, volume, log return, 5-day simple moving average (sma_5), and the weekday name by joining features (f) with weekday_meta (m) using the weekday column. USING(weekday) is a concise SQL shorthand that matches the column in both tables; it is equivalent to ON f.weekday = m.weekday. ORDER BY date LIMIT 10 sorts results by date and returns only the first ten rows — useful for quick previews.\npd.read_sql(q, con) and print(...)\nExecutes the SQL query against the SQLite connection and returns the result as a pandas DataFrame, which is then printed to stdout. Using pandas.read_sql is convenient for moving data between SQL and pandas for analysis or display.\nfinally: con.close()\nEnsures the SQLite connection is closed regardless of success or failure, releasing file handles and flushing changes.\n\nset -euo pipefail\npython3 scripts/make_sqlite.py\nShell-run explanations:\n\nset -euo pipefail\nA shell safety idiom that makes scripts fail-fast: -e exits on any non-zero command status, -u treats unset variables as errors, and -o pipefail causes a pipeline to fail if any component fails. While not SQLite-specific, it helps ensure the Python script’s failures are propagated to the shell.\npython3 scripts/make_sqlite.py\nRuns the Python script that creates/populates db/synth.db and executes the example SQL query. After running, check db/synth.db with SQLite tools (for example sqlite3 db/synth.db or GUI clients) to inspect the features and weekday_meta tables and to run ad-hoc queries.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>05 — SQL & SQLite (Python)</span>"
    ]
  },
  {
    "objectID": "06_pandas_joins_parquet.html",
    "href": "06_pandas_joins_parquet.html",
    "title": "7  06_pandas_joins_parquet.qmd",
    "section": "",
    "text": "Introduction — This page demonstrates using pandas to join a lookup (metadata) table into a processed DataFrame and writing the result to Parquet. Explanations focus on the pandas join/merge semantics, dtype adjustments for efficient storage, and writing Parquet files.\n\nfrom pathlib import Path\nimport pandas as pd\n\nPath(\"data/processed\").mkdir(parents=True, exist_ok=True)\ndf = pd.read_csv(\"data/processed/synth_features.csv\", parse_dates=[\"date\"])\nif \"weekday\" in df.columns:\n    df[\"weekday\"] = df[\"weekday\"].astype(\"int8\")\n\nmeta = pd.DataFrame({\n    \"weekday\": [1,2,3,4,5,6,7],\n    \"weekday_name\": [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n})\ndf = df.merge(meta, on=\"weekday\", how=\"left\")\n\nif \"price\" in df.columns:\n    df[\"price\"] = df[\"price\"].astype(\"float32\")\nif \"volume\" in df.columns:\n    df[\"volume\"] = df[\"volume\"].astype(\"int32\")\n\nout = \"data/processed/synth.parquet\"\ndf.to_parquet(out, index=False)\nprint(df.head())\nimport os\nprint(\"\\nParquet file size (bytes):\", os.path.getsize(out))\n\n        date       price  volume  log_return      sma_5  weekday weekday_name\n0 2022-01-01  100.496696     748    0.000000  100.49670        6          Sat\n1 2022-01-02  100.358398     417   -0.001377  100.42755        7          Sun\n2 2022-01-03  101.006104     776    0.006433  100.62040        1          Mon\n3 2022-01-04  102.529198     324    0.014967  101.09760        2          Tue\n4 2022-01-05  102.294998     918   -0.002287  101.33708        3          Wed\n\nParquet file size (bytes): 7994\n\n\nThe above code:\n\npd.read_csv(..., parse_dates=[\"date\"])\nReads the CSV into a DataFrame and converts the date column to datetime64[ns]. Parsed datetimes enable correct sorting, time-based operations, and compact storage in Parquet.\ndf[\"weekday\"] = df[\"weekday\"].astype(\"int8\")\nEnsures the weekday column uses a small integer dtype before the join. Choosing a compact integer (int8) reduces memory use and results in smaller on-disk Parquet files.\nmeta = pd.DataFrame({...}) and df.merge(meta, on=\"weekday\", how=\"left\")\nCreates a lookup table (meta) mapping weekday integers to names, then performs a left join of df with meta on the weekday column. A left join preserves all rows from the left table (df) and adds matching columns from meta; rows without a match receive NaN in the joined columns. Using on=\"weekday\" explicitly names the join key; how=\"left\" selects left-join semantics.\ndf[\"price\"] = df[\"price\"].astype(\"float32\") and df[\"volume\"] = df[\"volume\"].astype(\"int32\")\nDowncasts numeric columns to narrower dtypes to reduce memory and Parquet size. float32 is typically sufficient for many price series and halves memory compared with float64. Choose dtypes that retain the required precision for downstream analysis.\ndf.to_parquet(out, index=False)\nWrites the DataFrame to a Parquet file. Parquet stores data column-wise and is efficient for both storage and columnar reads. index=False prevents pandas from writing the DataFrame index as a separate column, keeping the file tidy. Parquet writing depends on an available engine (commonly pyarrow); ensure an engine is installed in the environment when running this code.\nos.path.getsize(out)\nPrints the Parquet file size in bytes as a quick round-trip check to confirm the file was written and to inspect storage impact after dtype optimizations.\n\npython3 - &lt;&lt;'PY'\nfrom pathlib import Path\nimport pandas as pd\nPath(\"data/processed\").mkdir(parents=True, exist_ok=True)\nprint(\"Ensure data/processed/synth_features.csv exists, then run the Python snippet above in this page to create parquet.\")\nPY\nThe inline shell/Python snippet above runs a short Python interpreter session (here-doc) that reminds the user to ensure the source CSV exists before running the main snippet. It does not perform joins or Parquet writes itself; it is a convenience check to avoid running the main snippet when the input file is missing.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>06 — pandas Joins & Parquet</span>"
    ]
  },
  {
    "objectID": "07_html_scraping.html",
    "href": "07_html_scraping.html",
    "title": "8  07_html_scraping.qmd",
    "section": "",
    "text": "Introduction — This page demonstrates a minimal, well-behaved HTML scraping pattern that first checks robots.txt, caches the fetched HTML, uses a clear User-Agent, and parses the page with BeautifulSoup. Explanations below focus on respectful scraping practices, the robotparser check, request headers/timeouts, polite pauses, caching, and basic parsing to extract the page title and links.\n\nfrom pathlib import Path\nimport time\nimport requests\nfrom urllib import robotparser\nfrom bs4 import BeautifulSoup\n\nURL = \"https://example.com\"\nRAW = Path(\"data/raw/page.html\")\nPath(\"data/raw\").mkdir(parents=True, exist_ok=True)\n\nrp = robotparser.RobotFileParser()\nrp.set_url(\"https://example.com/robots.txt\")\nrp.read()\nuser_agent = \"STAT4160-student/1.0\"\n\nif not rp.can_fetch(user_agent, URL):\n    raise SystemExit(\"robots.txt disallows scraping this URL.\")\n\nif RAW.exists():\n    html = RAW.read_text(encoding=\"utf-8\")\nelse:\n    res = requests.get(URL, headers={\"User-Agent\": user_agent}, timeout=10)\n    res.raise_for_status()\n    html = res.text\n    RAW.write_text(html, encoding=\"utf-8\")\n    time.sleep(1)\n\nsoup = BeautifulSoup(html, \"html.parser\")\ntitle = soup.title.get_text(strip=True) if soup.title else \"(no title)\"\nprint(\"Page title:\", title)\nlinks = [(a.get_text(strip=True), a.get(\"href\")) for a in soup.select(\"a[href]\")]\nprint(\"First 5 links:\", links[:5])\n\nPage title: Example Domain\nFirst 5 links: [('Learn more', 'https://iana.org/domains/example')]\n\n\nExplanation for the above code:\n\nrobotparser.RobotFileParser() / set_url(...) / read()\nCreate a robots.txt parser instance, point it at the site’s robots.txt, and load/parses its rules. This produces the rule set used to decide whether a particular User-Agent may fetch a particular URL.\nuser_agent = \"STAT4160-student/1.0\" and rp.can_fetch(user_agent, URL)\nSupply a clear, descriptive User-Agent that identifies the client. can_fetch checks whether the named user-agent is allowed to access the given URL per the site’s robots.txt. If can_fetch returns False, the script exits immediately — scraping is aborted to respect the website’s policy.\nCaching with RAW.exists() / RAW.write_text(...)\nIf a local copy of the page already exists at data/raw/page.html, the script reads the cached HTML instead of re-downloading. Caching reduces load on remote servers during development and repeated runs. When no cache exists, the script performs a single GET and writes the body to the cache file.\nrequests.get(..., headers={\"User-Agent\": user_agent}, timeout=10) and res.raise_for_status()\nThe User-Agent header is sent with the request so servers can identify the client. timeout=10 avoids hanging indefinitely on slow servers. raise_for_status() raises an exception for 4xx/5xx responses so failures are explicit and can be handled (or cause the script to exit under set -e).\ntime.sleep(1) — polite pause\nAfter fetching and saving a page, the script sleeps briefly to avoid sending requests in rapid succession. Respectful scraping typically includes pauses between requests; the appropriate delay depends on the target site and any Crawl-delay directives (see note below).\nBeautifulSoup(html, \"html.parser\"), soup.title, and soup.select(\"a[href]\")\nParse the HTML into a navigable DOM with BeautifulSoup. Extract the page title (if present) via soup.title.get_text(...). Use soup.select(\"a[href]\") to select anchor tags that have an href attribute and build a list of (text, href) tuples for simple link extraction.\nSafety and robustness notes:\n\nAlways obey robots.txt as an explicit first step. robotparser helps automate this check, but be aware that robots.txt is advisory — responsible scrapers must still follow it.\nSome robots.txt files include Crawl-delay directives; Python’s robotparser may not support all non-standard fields. If a Crawl-delay is present, respect it by increasing sleep intervals accordingly.\nUse descriptive User-Agent values and provide contact information when appropriate (e.g., \"my-bot/1.0 (contact: email@example.com)\") so site operators can reach you if needed.\nHonor rate limits, avoid aggressive parallel requests, and prefer cached reads during development to minimize load on target sites.\nHandle relative URLs carefully when following links: use urllib.parse.urljoin(base, href) to form absolute URLs if you will fetch linked pages.\n\n\nset -euo pipefail\npython3 scripts/scrape_example.py\nExplanation for the above code:\n\nset -euo pipefail makes the shell session fail-fast so that errors in the Python script are visible and stop subsequent commands.\npython3 scripts/scrape_example.py runs the scraper. Under the checks in the Python code, the script will stop immediately if robots.txt disallows scraping the target URL.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>07 — HTML Scraping (Respect robots.txt)</span>"
    ]
  },
  {
    "objectID": "08_pytest.html",
    "href": "08_pytest.html",
    "title": "9  08_pytest.qmd",
    "section": "",
    "text": "Introduction — This page contains simple pytest tests that validate processed CSV/Parquet artifacts and demonstrate how to run the test suite from the repository root. Explanations below describe what each test asserts and how the test run is invoked.\n\nimport os\nimport pandas as pd\n\ndef test_synth_features_columns_and_no_nans_price():\n    df = pd.read_csv(\"data/processed/synth_features.csv\")\n    expected = {\"date\",\"price\",\"volume\",\"log_return\",\"sma_5\",\"weekday\"}\n    assert expected.issubset(set(df.columns)), f\"Missing expected cols: {expected - set(df.columns)}\"\n    assert df[\"price\"].isna().sum() == 0\n\ndef test_date_monotonic_in_parquet():\n    df = pd.read_parquet(\"data/processed/synth.parquet\")\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    assert df[\"date\"].is_monotonic_increasing, \"Date column is not monotonic increasing\"\n\ndef test_parquet_exists_and_size():\n    path = \"data/processed/synth.parquet\"\n    assert os.path.exists(path), \"Parquet file missing\"\n    assert os.path.getsize(path) &gt; 0, \"Parquet file exists but is empty\"\n\nExplanation for the above tests:\n\ntest_synth_features_columns_and_no_nans_price()\n\nLoads data/processed/synth_features.csv into a DataFrame.\n\nVerifies the presence of the expected columns: date, price, volume, log_return, sma_5, and weekday. If any are missing the assertion fails and the failure message shows which columns are absent.\n\nVerifies there are no missing (NaN) values in the price column; the test fails if any NaNs are present. These checks ensure the processed CSV has the required schema and that the price column is complete.\n\ntest_date_monotonic_in_parquet()\n\nReads data/processed/synth.parquet into a DataFrame and ensures the date column is parsed as datetimes.\n\nAsserts that df[\"date\"].is_monotonic_increasing is True, i.e., the date column is strictly non-decreasing down the rows. This guards against unordered or shuffled time-series data which can break time-based analyses.\n\ntest_parquet_exists_and_size()\n\nAsserts that the Parquet file data/processed/synth.parquet exists on disk.\n\nAsserts the file size is greater than zero (not empty). This is a basic sanity check that a previous processing step successfully wrote the Parquet artifact.\n\n\nNotes on test design: - Tests use straightforward, fast I/O checks and pandas semantics to assert correctness; keep tests small and deterministic to make them reliable in CI. - Assertion messages provide actionable feedback (e.g., which columns are missing) to speed debugging when tests fail.\nset -euo pipefail\npytest -q\nExplanation for the above commands:\n\nset -euo pipefail is a shell safety idiom used when running the test command from a shell script or CI step:\n\n-e exits immediately if any command returns a non-zero status.\n-u treats unset variables as errors.\n-o pipefail causes a pipeline to return a non-zero exit if any component fails. Using this ensures the test run fails loudly and stops subsequent shell commands when a problem occurs.\n\npytest -q runs the pytest test discovery and execution in quiet mode (-q), which produces concise output (dots or short summaries) suitable for CI logs or quick local runs. Run this from the repository root so file paths like data/processed/synth_features.csv and data/processed/synth.parquet resolve correctly.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>08 — pytest (Simple Tests)</span>"
    ]
  },
  {
    "objectID": "09_pytorch_miniloop.html",
    "href": "09_pytorch_miniloop.html",
    "title": "10  09_pytorch_miniloop.qmd",
    "section": "",
    "text": "Introduction — Minimal example showing how to implement a small supervised training loop in PyTorch. The snippet covers dataset creation, a tiny GRU-based model, device selection (CPU/GPU), optimizer and loss setup, and the standard training loop (forward, backward, gradient clipping, step). Explanations below describe the PyTorch-specific components and the role each plays in training.\n\n# Minimal PyTorch example (runs on CPU if CUDA unavailable)\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\ntorch.manual_seed(0)\nnp.random.seed(0)\n\nT, F, N = 16, 3, 512\nX = np.random.normal(0, 0.5, size=(N, T, F)).astype(\"float32\")\nw = np.random.normal(0, 0.2, size=(F,)).astype(\"float32\")\ny = (X[:, -1, :] @ w + np.random.normal(0, 0.1, size=(N,))).astype(\"float32\")\n\nds = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\ndl = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n\nclass SmallGRU(nn.Module):\n    def __init__(self, F, H=32):\n        super().__init__()\n        self.gru = nn.GRU(F, H, batch_first=True)\n        self.head = nn.Linear(H, 1)\n    def forward(self, x):\n        _, h = self.gru(x)\n        return self.head(h[-1]).squeeze(-1)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet = SmallGRU(F).to(device)\nopt = torch.optim.AdamW(net.parameters(), lr=3e-3)\nloss_fn = nn.L1Loss()\n\nfor ep in range(1, 11):\n    net.train()\n    tot = 0.0\n    for xb, yb in dl:\n        xb, yb = xb.to(device), yb.to(device)\n        opt.zero_grad(set_to_none=True)\n        yhat = net(xb)\n        loss = loss_fn(yhat, yb)\n        loss.backward()\n        nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n        opt.step()\n        tot += loss.item() * xb.size(0)\n    print(f\"epoch {ep:02d} | train_mae={tot/len(ds):.6f}\")\n\nExplanation for the above code:\n\ntorch.manual_seed(0) and np.random.seed(0)\nSet deterministic seeds for PyTorch and NumPy random generators to make the example reproducible during development and demos. Full determinism across platforms requires additional PyTorch flags and may still vary by backend.\nData creation and TensorDataset / DataLoader\nThe example synthesizes features X and targets y with NumPy, converts them to PyTorch tensors, and wraps them in a TensorDataset. DataLoader provides minibatch iteration (batch_size=64) and shuffling (shuffle=True), which is essential for stochastic optimization.\nclass SmallGRU(nn.Module):\nDefines a compact model with a single GRU layer and a linear head. nn.GRU(F, H, batch_first=True) processes sequences with F features per time step and returns a hidden state of size H. The model’s forward returns a single scalar per sample by taking the final hidden state (h[-1]) and applying self.head.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") and .to(device)\nSelects GPU if available; otherwise uses CPU. Moving the model and tensors to the same device is required for computation. This pattern makes the script portable across machines with and without CUDA.\nopt = torch.optim.AdamW(net.parameters(), lr=3e-3)\nUses the AdamW optimizer (Adam with decoupled weight decay) for parameter updates. AdamW is a sensible default for many models; learning rate and optimizer choice depend on the task.\nloss_fn = nn.L1Loss()\nL1 loss computes mean absolute error (MAE) between predictions and targets. It is robust to outliers compared to squared losses and is a common choice for regression tasks where MAE is a meaningful metric.\nTraining loop structure:\n\nnet.train() sets the module to training mode (affects layers like dropout or batch norm).\nFor each batch: move inputs/targets to device with .to(device).\nopt.zero_grad(set_to_none=True) clears gradients; set_to_none=True can be marginally faster and uses slightly less memory.\nyhat = net(xb) performs the forward pass.\nloss = loss_fn(yhat, yb) computes the batch loss.\nloss.backward() computes gradients via backpropagation.\nnn.utils.clip_grad_norm_(net.parameters(), 1.0) applies gradient clipping by global norm to avoid exploding gradients and stabilize training.\nopt.step() updates parameters using computed gradients.\ntot += loss.item() * xb.size(0) accumulates the sum of per-example losses to compute epoch MAE.\n\nEpoch reporting: tot/len(ds)\nAfter iterating batches, dividing the accumulated (sum) loss by dataset size yields the mean absolute error across the epoch. Printing this each epoch provides a simple training progress metric.\nPortability and small-scale notes:\n\nThe example runs fine on CPU for demonstration and on GPU if available. For real training with larger models or data, prefer proper device-aware data loading, mixed precision, and more robust reproducibility settings.\nFor deterministic behavior across runs and platforms, additional configuration is required (e.g., torch.use_deterministic_algorithms(True)), but this can reduce performance or rely on backend support.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>09 — PyTorch Minimal Training Loop</span>"
    ]
  },
  {
    "objectID": "10_quarto_book.html",
    "href": "10_quarto_book.html",
    "title": "11  10_quarto_book.qmd",
    "section": "",
    "text": "Introduction — Commands for rendering a Quarto project located in book/. Examples show rendering the entire site, a single page for fast iteration, previewing the site locally, and publishing to GitHub Pages using Quarto’s helper command. Explanations describe what each command does and practical notes for common workflows.\nquarto render book\nRenders the entire Quarto project found in the book/ directory and writes the output into the project’s output folder (commonly book/_site or book/_freeze depending on project settings). Use this when you want a full, reproducible build of the site for publishing or final review.\nquarto render book/05_sql_sqlite.qmd\nRenders a single page (book/05_sql_sqlite.qmd) rather than the whole site. This is useful for fast iteration during authoring because it rebuilds only the specified document and its immediate dependencies, reducing turnaround time compared with rendering the entire book.\nquarto preview book\nStarts a local preview server for the Quarto project in book/. The preview server serves the rendered site on a local HTTP address (for example http://127.0.0.1:4200) and typically supports live reload so changes to source files are shown automatically. Use this for manual checks and interactive review before publishing.\nquarto publish gh-pages --project=book\nPublishes the Quarto site to GitHub Pages using Quarto’s built-in helper. This command packages the book/ project and pushes it to the gh-pages branch (or config-specified branch) on the repository’s remote. Requirements and notes: - A Git remote must be configured and accessible from your environment (e.g., origin set to git@github.com:your-username/your-repo.git or HTTPS). - The invoking account must have permission to push to the target branch on the remote. - Quarto’s publish helper automates common steps but you may prefer manual deployment if you need custom deployment steps or CI-based publishing. - For CI environments, consider running quarto render book in a pipeline and pushing the generated output to the appropriate branch with secure credentials rather than using quarto publish interactively.\n```yaml project: type: book\nbook: title: “Tools in Action — Demo” author: “Your Name” chapters: - index.qmd - 01_git.qmd - 02_linux_basics.qmd - 03_shell_essentials.qmd - 04_makefile.qmd - 05_sql_sqlite.qmd - 06_pandas_joins_parquet.qmd - 07_html_scraping.qmd - 08_pytest.qmd - 09_pytorch_miniloop.qmd - 10_quarto_book.qmd\nformat: html: theme: default",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>10 — Quarto Book & Rendering</span>"
    ]
  }
]